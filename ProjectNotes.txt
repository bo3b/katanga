Katanga (maybe release name is Screen3d, Scr33d) is the virtual 3D screen.

Any given 3D game running on NVidia or SBS will take the bits and put them into
the screen Quad in the Unity environment.

The performance inside the injected game must be as fast as possible to avoid
CPU hits that can be very costly.  
The performance inside the Unity display needs to be fast at least 90Hz, to 
avoid any VR lag.  This can be simple scene, and will be on different CPU cores.
Sync problems will not exist, because we'll copy bits from the game to IPC, which
will then be put to the Quad. Since the headset only draws every 1/90th second,
there should be no tearing, because it will always fetch the newest image.


The Katanga project is multi-component.

1) The Unity app itself, which draw in the VR headset.
2) The UnityNativePlugin, which is native C code called from Unity.
3) The destination Deviare plugin, which will be injected into the game.

The Unity app is x64 only, because there is no point in a x32 version, since VR 
requires x64.  The Deviare plugin will be x32 and x64 depending upon the game.

DX9 and DX11 and OpenGL should all be supported with the Deviare Plugin.


Trying to decide whether to use Deviare2 itself, or to use the in-proc version only.
Not completely clear what would be best, and Deviare2 is fairly confusing and
poorly documented.  Lots of funny pieces that are not clear, like their Agent, and
active plugins. Plugins can be hook specific and native.

The code for the Unity project itself must include a C++ chunk, because we need
to call the OpenVR code, which is native C++.  Maybe this is handled by the Unity
plugin code, as we enable VR and it activates, but we still need access to the 
buffer for the quad/screen.

We don't need to hook remotely, hooking from inside the running game is OK.  And we
really only need the Present() call.  There also does not seem to be a particularly
good callback mechanism. They have OnFunctionCalled, but that is before we finish
copying the backbuffer, and we need a more direct IPC of some form.

Probably we need to use memory mapped file as the IPC, so that the backbuffer copy to
the Unity app is fast.  We can also use that for notification of new data.

So, all in all, doesn't seem worth using Deviare over in-proc, but maybe.


Back with more thoughts.  Given that the Quad we are drawing into is managed by Unity
it seems to push toward using Deviare2 directly, not in-proc.  Unity is all C#, and 
it is harder to do in native plugin.

So, for starting at least, current plan is to not use native plugin, and do all the 
work in C#, including Deviare, and hooking the Present call.

The actual code for the game itself will be a Deviare native plugin in C++, that will
do the work of fetching the backbuffer and copying it for the Unity app to use.  That
code needs to be as fast as possible, to avoid impacting the game's CPU use.  


Starting down this path, ran into some weird Unity problems.  Was getting a missing file
error for the use of Nektra.Deviare2.dll, missing a stdole file that was 1.1 version.  
Not sure I understood all that, because the file exists in the right spots on Win10, but
was not being seen.  Making a copy from the interop files in Program Files(x86) and
dropping that into the Unity root worked to solve that error.  Doesn't seem releated to
Nektra, seems related to Unity.

Using the Nektra.Deviare2.dll from the 2.8.3 binary release.  I was able to build my own
nektra libs and changed params from XP support, but seems to be unnecessary to do all that.

Also required- regsrvr32 DeviareCOM64.dll.  Unity runs in x64 now, and the creation of
the NktSpyMgr was crashing with a COM exception because it was not registered.  This is 
possibly problematic, because we'll need this on target systems.

With those in place, the Unity C# code calling to NktSpyMgr actually works.

Right now it's setup for VS2017, can move back to VS2013 if that seems better.  Right at 
the moment, I'm going to keep it this way, because the Unity->VS debugging is already
setup and working for breakpoints.


Deviare CTest is a good example of similar behavior.  Uses the built in Agent, not needing
the custom plugin until performance is needed. Patches all hooks, works in VS, including
debug through their code.

Requires several pieces that were not obvious.  Both Deviaredb and db64.  DeviareCom64.dll 
because it's a 64 bit app.  dvagent.dll for x32 apps, dvagent64.dll for x64.


10-9-17
Mostly working in a basic format.  All hooking is working into game.  

App to built in Unity needs to be DX11 target and x64 only.  Oculus Rift requires that, 
so there is no point in making any conversions to other APIs.  

Basic strategy will be to create our game snapshot texture as a DX9EX Texture2D, because
that can share with DX11 easily, and also with DX9 easily. Works both way, so DX9 game
works, DX11 game works.  Destination is a DX11 Texture2D in Unity.

Testing a bare minimum Unity project seemed to reqired 10% of the GTX 980 GPU, when in 
medium quality mode.  On High quality, it was 22%.  There is some complexity in terms of
vsync and driver defaults, so best bet will be to profile it when ready.

Sticking with the DX9 hooking for now, because it's already working, and needs to be done
in any case.  DX11 is more familiar, but requires DXGI::Present hook instead.  However,
the GameSurface will be a DX9EX variant, in order to be shareable.

The destination copy texture must be a RenderTarget, because of driver restrictions. It
cannot be a simple surface.  Also, for sharing, it needs to be a RenderTarget because the
backbuffer is always a RenderTarget and they must match in order to share.
https://msdn.microsoft.com/en-us/library/windows/desktop/bb174471(v=vs.85).aspx


10-17-17
OK, turns out that the MSDN docs half-lie and it is not possible to use surface
sharing on any DX9 device.  Only DX9Ex devices will work.  Confirmation here:
https://www.gamedev.net/forums/topic/638495-shared-resources-eg-textures-between-devicesthreads/
And, also confirmed using the DX9 debug layer on Win7 setup.  When trying to use any
variant of DX9 and a surface return handle, it would put up the error:
Direct3D9: (ERROR) :Device is not capable of sharing resource. CreateTexture fails.

Using the sample program, I confirmed that using the DX9Ex path works without any
error or notice using Debug layer.  Can also use CreateRenderTarget, no Ex, but that
seems of little value.  The errors looked like a bad parameter to CreateRenderTarget,
but in fact it was incorrect device.

This also points out a really important conclusion.  The DX9 and DX9Ex objects are 
not in fact the same.  Coercing from an Ex object will work, because it's a superset,
but going from DX9 to DX9Ex does not work, because the fundamental object is different.
Also confirmed by using QueryInterface, and getting E-NOINTERFACE.


10-19-17
Finally figured out what the crash was about.  Fuck! I say.  Fucking Microsoft has a
bad dxd9.h header file, been bad for over 5 years.  The dx9Ex interface is missing a
routine name in the IDirect3D9Ex interface section, of RegisterSoftwareDevice. That 
makes the C interface for IDirect3D9Ex, off by one.  So it calls the wrong routine.
Like GetAdapterMonitor instead of CreateDeviceEx. Fuck! Cost me two weeks on this one.


10-20-17
Experimenting with Console app, which just does CreateDX9, then CreateDevice, to see
what all needs to be an Ex object.  When CreateDeviceEx is used, we end up getting
a debug layer break in CreateTexture that the pool cannot be managed. Which leads
down a path of tweaking parameters.
However, when trying this to create a shared surface:
	hr = pDev9->CreateRenderTarget(1280, 720, D3DFMT_A8R8G8B8, D3DMULTISAMPLE_NONE, 0, false,
		&gGameSurface, &gGameSurfaceShare);
we get error:
Direct3D9: (ERROR) :Device is not capable of sharing resource. CreateRenderTarget/CreateDepthStencil fails.

Clearly the runtime requires an IDirect3DDevice9Ex.
And, since you cannot create a IDirect3DDevice9Ex with a DX9 factory, that means the 
top level CreateD39 must be Ex as well.  

And, in test app, if I do StretchRect from dev9 to dev9Ex, I get:
Direct3D9: (ERROR) :DstSurface was not allocated with this Device. StretchRect fails.
Which clearly indicates that we cannot go cross-device.  If the source object is the
dev9Ex, then it just crashes, debug layer is probably lame for Ex.


10-24-17
Getting closer.  With it not successfully creating and copying into a shared surface,
we now need to display this in Unity app.  The access to the surface there is not
terrific, and we also need to have it switch from DX9 to DX11 for the Unity display,
so the plan is to create a C++ unit for Unity side, and use their plugin model to
get access to C++ in this app as well.  Mostly this just needs to use the HANDLE to
the shared surface, and copy the bits into the in-Unity TV screen. 
Some questions about what needs to be write-only for performance, and how to sync
the copies, but let's get it limping first.

At present using the simple DX9 test app of Textures, because when I launch TheBall,
it fires some debug layer problems like CreateTexture needing to use only defaultpool.
Probably will need to hook and patch up some of these since we are putting the game
into an DX9Ex runtime, which changes some usage.

Restructering the project folder layout, because we need to have the next piece of
the UnityNativePlugin for C++ access.  Also, this layout was never right, Unity
rebuilds the .sln file for their piece at every open, so it's better if it's by 
itself and used as a subproject to a main project.   Doing this piecemeal to avoid
breaking the repo.


11-8-17
Bit of a ballbuster here, with Deviare crashing at launch instead of returning an error. 
This was happening on a full build, instead of while running in Unity. The problem wound
up being that the DeviairePlugin DLL path would change from Unity to the compiled app, so
yeah, thanks Unity for that kick in the nuts too.  There is no /Assets when the app is
compiled, which is fucking retarded.  

Then there is this crash in Deviare, which happens if the DLL is not found.  This is also
completely lame.  If it can't find it, it should return an error, not blow up.  If you see
this crash, it's because of the missing deviare plugin dll.

Crash:
	>	00000001()	Unknown
 	[Frames below may be incorrect and/or missing]	
 	DvAgent.dll!TNktArrayList<CNktDvParam *,128,TNktArrayListItemRemove_Release<CNktDvParam *> >::RemoveAllElements() Line 330	C++
 	DvAgent.dll!CNktDvHookEngine::Hook(CNktDvHookEngine::tagHOOKINFO * aHookInfo=0x8007007e, unsigned long nCount=1, int bIsInternal=0) Line 494	C++
 	DvAgent.dll!CDvAgentMgr::OnEngMsg_AddHook(tagNKT_DV_TMSG_ADDHOOK * lpMsg=0xffe16054, CNktDvTransportBigData * lpConnBigData=0xffe300e0) Line 2522	C++
 	DvAgent.dll!CDvAgentMgr::TAC_OnEngineMessage(CNktDvTransportAgent * lpTransport=0x77ad4060, tagNKT_DV_TMSG_COMMON * lpMsg=0xffe16054, unsigned long nMsgSize=1084, CNktDvTransportBigData * lpConnBigData=0xffe300e0) Line 699	C++
 	DvAgent.dll!CNktDvTransportAgent::WorkerThreadProc(unsigned long nIndex=5) Line 564	C++
 	DvAgent.dll!TNktClassWorkerThread<CNktDvTransportAgent>::ThreadProc() Line 169	C++
 	DvAgent.dll!thread_start<unsigned int (__stdcall*)(void *)>(void * const parameter=0xffe0418c) Line 115	C++
 	kernel32.dll!@BaseThreadInitThunk@12()	Unknown
 	ntdll.dll!___RtlUserThreadStart@8()	Unknown
 	ntdll.dll!__RtlUserThreadStart@8()	Unknown

Once all that is straight, and we use the Application.dataPath to get the proper path 
to the /Assets or /appname_Data folder, then we can properly build the full path to the 
DeviarePlugin.dll, and pass it to LoadCustomDLL.  Using the forward slashes returned
from Unity seems to work without any trouble.


11-15-17
Performance with OculusVR as the type XR Setting. Headset sensor activated.
Using NVidia Inspector graph. TheBall splash screen.  Anaglyph 3D Vision.
 Oculus: 41-45% of GPU
 OpenVR: 52-55% of GPU
 None:   24-26% of GPU
 
No game running, no SBS script, simplest scene.
 Oculus: 13-14% of GPU
 OpenVR: 20-21% of GPU
 
So, Oculus for SDK is clearly superior for performance.  The Stats part of Unity is misleading,
because for Oculus it shows full frame rate of 1/90 ms.  For OpenVR, it shows 0.5ms for Unity code. 
Different ways they handle end of frame.
It's also notable how much impact this has.  
For GTX 980, we jump from 25% for the game, to 45% for game+Rift.

LegacyShader/Diffuse, the default, is 2% GPU faster than Standard. 12-13% GPU.
Using sbsShader did not help any.

As another test, to see how much overhead Unity provides, running the Oculus sample
app, TinyRoom, release x64.  
 Oculus TinyRoom: 12%-13% of GPU.  
 
That app will use Oculus SDK directly, with native DX11 calls. No reason to 
think we can ever do better than that.


11-25-17
Seriously hard to get all this working, but sucess!  Now getting stereo bits from the
game, into Unity as a texture.  The display texture is side-by-side.  Took much time
and debugging to figure out how to get stereo bits out of DX9, no samples, bad docs.
Also that target surface for StretchRect cannot be shared, or it breaks the stereo.

Every stage of the pipeline has been unbelievably complicated to get working.  It's 
not a surprise no one else has tried this.  


11-28-17
Finally!  Got stereo bits from the game, all the way to the headset, and showing in
stereo in the virtual 3D TV.  Yes! 

Last trick here was in Unity, trying to sweettalk the VR support into showing my SBS
image as actual stereo, half for each eye.  Currently not quite right, it's requiring
multipath support, and I really want SinglePass Stereo for performance.  But, this is
working, with multipath, and an OnPreRender call for the camera script.  That gets
called once for each eye, so I can alternate eyes for the CopyTexture call.

Using CopyTexture seems superior to Blit, because Blit requires a shader to run, and
CopyTexture just does something closer to a memcpy.  May not matter at all, because
ultimately something probably runs a shader to put bits into the quad, but I think
this might save a Draw call from shaders.  Probably does not matter, not measurable.

Performance with a full Release buid is good.  Game runs well, if it's full screen
it's at full 60 fps. No F notification in VR, very smooth.  55-70% of GPU in use.
Some stalls during flyover in game.  


11-30-17
And... Winner!  Got it to show stereo all the way to the virtual 3D TV.  Correct eyes,
changeable depth and convergence like you'd want.  Looks good.

The key aspect here was working out the best way to do this.  The best way seems to
be to have a custom shader attached the the Quad Material, which does the work of 
copying the texture to the actual VR screen and transforming them through the MVP
matrix, as the head camera moves.  This saves an extra copy of the bits, and that
stage has to run anyway, so modifying that shader seems like best.  While there,
it has the ability to use the primary input texture of _bothEyes, and decide to
fetch either left or right half depending upon the unity_StereoEyeIndex variable.
Changes which half of the texture bits are fetched from, and thus shows the correct
piece to the correct eye.

We specifically do not use the Graphics.Blit, because that required a script on the
camera in order to function, and it wasn't clear if would work to handle both eyes.
It would also be another copy that is unnecessary.
Similarly, we don't Graphics.CopyTexture to the Quad Material, because there needed
to be two copies, and had to be done from the camera script as well, where the
OnPreRender would get called twice, so we could swap eyes.  Also a second copy, 
although this does work when using Multi-Pass Stereo.  In SinglePassStereo, OnPreRender
is only called once, so this won't work for the faster path.  
Tried to work out how to use the vrDesc for a RenderTexture, but nothing made
sense, and could not get anything to work.  It was documented as automatically
working for built in shaders, but it never seemed to recognize it was a VR texture.

This is also using SinglePassStereo.  This is supposed to be quite a lot faster,
and we don't need multipass.  Successfully drawing in VR using SinglePassStereo, using
the custom shader.

This is going to be nearly as fast as it is possible to make it.  The bits from the 
game are copied only twice, once to get stereo bits cleanly out of game, then once
again to the shared texture.  The shared texture is used directly in the DX11 side,
by the shader to fetch pixels.  So, very little extra copies.  Would be nice to lose
that second copy, but NVidia doesn't work copying to a shared resource.

Overall performance is good. GPU usage is fairly low, 38-42% GPU on the splash screen.
There are some F frame rate indicators in VR that are a concern.  Probably blocking
VR from multithreaded access causing stalls, but don't really know.  Need to look.
Ths is using 3D Vision to screen, which might be locked to 60Hz, and stall when
lower.  We get stalls during Ball flyover.  Oculus F indicator up constantly when
the ball is close with pull effect.  With that effect on, GPU usage goes to 70-74%.
Idling in-game is 57-60%.

No crashes.  Very solid.


12-3-17
Looking at a dropped Compositor problem, where I see C show up periodically.  Should
not be happening, we have maybe 50% of GPU headroom.  Tried backing up from the
extra thread to do second copy, and that did not help, so extra thread is not the
problem.  
Lots of other tests here, cannot quite pin it down.  Almost seems like a false
error, although Oculus debug tends to be very good.  No other tool shows a snag
when the ball is transparent up close, where in headset we see flashing F.  Tried
VS profile, Unity GPU profile.  And multiple paths of dropping pieces of my 
pipeline.  Only change that mattered was drawing the shared surface, even if bits
were not changing.
If not in the full screen foreground, I get Fs.  If it's full screen, but not front
app, still get frames through, but no Fs.
Changing vsync in game, to limited to 160 using .ini.  Works.  Game runs at 105 fps
natively.  Still getting Fs.

Turning off shared surface altogether, and setting the Textures to null still gives
me Fs.  So even with no share at all between apps, it still stalls.  This suggests
that it has something to do with the GPU itself, where the dual GPU processes stall
the pipeline, or can't switch effectively or something.
Definitely seems related to the transparency effect.  When I set ball to not transparent
it doesn't F, in windowed mode.  Game specific?  Might be pipeline flush on transparent
or something.

After doing quite a bit of analysis, including using GPUView to look at GPU usage while
both apps are running- the problem is that TheBall has a sequence that takes 8ms of time
and locks out anything else from getting GPU time during that sequence. When this overlaps
the Present for Katanga, it stalls and shows a dropped F frame. 
The actual underlying problem is that the GPU scheduler is retarded, and apparently 
cannot be tuned.  It decides unilatterally that the frontmost window TheBall is more 
important than anything else and thus does not give up time when our much more critical
VR app calls.  SetGPUPriority does nothing, and there are no nvapi calls that would
allow a fix. Scheduler is a black box and fuck off.


12-6-17
Interesting experiment where I only create a DX9Ex factory, but then allow the calls
to go through as normal DX9 calls for CreateDevice, CreateTexture, Present et. al.
Creating the Shared resource still works, no error, and I get stereo out.
This is probably the best way to go, to avoid having to tweak all the other calls for
DX9, like CreateTexture, CreateTexure3D to add parameters.
This works with only DX9Ex factory, but the returned CreateDevice is still considered
a Device9Ex by the debug runtime, and I still need all the CreateTexture/Buffer overrides
to fix those debug flaws.


5-18-18
Pretty big gap there as I lost motivation.  Back looking at the dropped frames problem.
Definitely seems to be priority related, but I have no tools to fix that. Asked for
access to Context Priority in the NVidia VRworks, but no one responds, including Dave.

Managed to create a semaphore based stall system, using a Windows Event that can be
triggered on/off.  In the main VR app, I trigger this off after 3ms from the front of
the frame, then in the game, at patched draw calls, I look for and stall if that is
off.  This works in that the code does what I expected, and doesn't seem to have any
bugs.  In GPUView I can see that the game 8ms blob is broken into two pieces and the
frame rate in the game drops to 45.  

Not a full solution though.  Still get occasional dropped frames as it conflicts.
It would be possible to tune it for just this game, but that doesn't seem like a 
particularly hot strategy anyway.  

Switching now to embedding the DX11 VR side into the game app as well, and doing the 
entire VR world on my own handbuilt code, no Unity.  This gives me the most flexibility,
with the most work as well.  Worth an experiment to see if I can keep performance up
this way.  Experiments with VorpX suggest that he is doing this, and his performance 
seems spot on.

Still going to use Unity app for the moment, as all the launching and Deviare injection
happen there.  Just turning off the VR aspect, so as far as it is concerned, it's just
a regular 2D game.


8-5-18
Installing for new Squanchando computer.  Decided to update the VS2017 and latest LTS
Unity.  Reasoning that it's better to have latest graphical debugging tools here, as
more valuable than stable setup.  Causes a few problems, like broken directory paths.

Also of note, doesn't work at all on WMR, because WMR does not presently support 
3D Vision.  Driver crashes if 3D is enabled.  Bug reported.


8-27-18
Tried to get Registration-Free COM to work, but cannot quite get the right combo for
the Unity app.  Too strange a runtime, not clear where to connect.  Got it working
OK for InvisibleWalls sample, including a subdirectory of DeviareCOM, but nothing 
else.  Does not appear to support an arbitrary directory. Documentation sucks, and I
could not find any examples of people using a different directory than the root.
Even using the root for Unity does not work, probably because they set the working
directory to PlugIns or something.  Super fragile mechanism, no good debug tools.
The sxstrace command creates an empty log in the Unity case.  All in all, a good
waste of multiple weeks.

Change of plan there- RegFree is a nice-to-have, not required.  Definitely far 
superior, but tweaky and hard to figure out.  Easy enough to regsvr32 at install 
time instead, so let's skip the distraction and get on more important stuff.

Like avoiding the Steam sublaunch for hooking.  If I follow:
https://stackoverflow.com/questions/9624629/debug-games-from-steam-with-pix
and create a steamapp_id.txt file with 35460 in it, I can successfully launch
the Steam version of TheBall without any problems.


10-7-18
Switching gears here.  Tried to hookup to Psychonauts, and was successful at running
in VR as well.  Then tried to add HelixMod and discovered the old versions do not
support the CreateDeviceEx call, so when it's installed, there is no hook available.
Tried to work around it by calling out to System32 specifically, but something is
interfering with HelixMod, or it is, and no longer hooks the game and fixes the shaders.
This combined with all the tweaks needed to get Ex variant working seems like a hint
this isn't the right approach.  

Also worth noting, using the tweaks to different calls may not work properly under
HelixMod, because of his own hooks.  So, the workarounds could introduce problems.
Could make an automatic upgrade to last version HelixMod however.

This seems like as good a time as any to try to use the Encode/Decode chip that is 
built into all GPUs now.  We can convert the game data to a video stream, and pass
that to the VR/Unity side to Decode.  Pretty sure this is how BigScreen works.
It can work lossless video, which was a big concern.  Only real question is whether
it is fast enough.  Probably not as good as surface sharing, but maybe that doesn't 
matter.  Should be more reliable and simpler to implement.  Should also make it 
much easier to connect with DX11 games, as the decode part would not change.

Hopefully the encode/decode does not require going to system memory, which would
be a big killer to this approach.


11-7-18
Setting up to use nvcodec is challenging, because of the many, many versions.  The 
latest version is probably not the right choice, because it requires an up to date
driver.  As of the moment, SDK 8.2 requires driver 397.93 or higher.  That's quite a
bit higher than we'd want to require, because older cards and games can run better
on older drivers.  Sometimes we need those older drivers.
Using that idea, going back to SDK 6.0 is not unreasonable.  Requires driver 358.xx or 
higher, which is after GTX 980ti, but before GTX 1080.  Not as far back as we'd like,
but better than nothing.  SDK 5.0 requires driver 347.09 or better, but also requires
installing the CUDA toolkit, which we'd rather not do.  
SDK 6.0 supports lossless encoding, which is the main goal.

So for the moment, we are going with SDK 6.0.  Might change depending upon user 
requirements.


12-19-18
Got a basic structure setup for using nvcodec, but it doesn't work.  Seems to require
a source buffer of an OffscreenPlainSurface, which can be on the video card memory,
but does not allow any normal ways of updating the surface with game bits.  StretchRect
will not work with Offscreenplain as the destination.  Seems to only be viable for
loads from system memory, which is pointless.
The DX9 interop code does use an alternate path though, using a Media Foundation Layer
from Microsoft, with a DXVA2 library.  Trying that path now.

One strange thing is launching Steam games. The steam launcher will often interfere, and
throw up the error 5:65432 error.  This can be bypassed most times with a file named
'steam_appid.txt' with the appid in the file.  Doesn't seem to work with The Ball though.


1-7-19
Got basic code working, and can see an encoded sample program.  Backed up a bit, and 
integrated the Katanga code into the StereoUnproject sample, as a test of encoding a
DX9 stereo backbuffer.  The code works, and encodes a video to disk.  Colors are wrong, 
but basic data is there, and probably something off with the interminable settings in
the nvcodec.  This uses the DXVA2 interface, which apparently is required to allow us 
to copy the backbuffer onto something the nvcodec can accept.  Using StretchRect is not
possible.  Hard.

Next up is decoding the stream.  As near as I can tell, it requires the use of Cuda, and
cannot use a DXVA2 variant.  Giant mess here, seems like only one path actually works, the
rest is just broken.  Begs the question of using Cuda though- if we can share via a 
Cuda surface, there is no need to use the encode/decode.  Still in investigation after 
2 years of development.  Christ.  


Also worth noting that the performance problem seen above, happens because the VR app is
in the background.  That's allowing the game to block the VR app and cause stalls.  Probably
not an issue, but might be for more demanding games.  Could be solved by putting the VR
generation in the game process space itself, and not use Unity.  Might be necessary and/or
worthwhile.  

This is a separate problem from getting bits to the VR app.  That requires surface sharing
or the video encode/decode, or possibly cuda.  That needs to be solved regardless.


1-11-19
Tested another scenario that is worth writing down as a possible solution.  While setting up
for the DVAX2 surface sharing, it's surprising that it allows the nvcodec to access the
surface, and seems to be the way the samples all work.  So, I made a test case (checked-in) of
using a decode destination surface as the target for a StrechtRect, then take that surface
and StretchRect back to the backbuffer.  This works.  This was with standard DX9 factory, not
the DX9Ex factory, so would require no changes to the game setup.

So, that means it should be possible to use this as a sharing technique, without needing to
modify everything for DX9Ex.  Might be useful.

Still, does not seem like the way to go, because we actually need to go from DX9 to DX11 in 
order to draw in VR.  This DXVA2 buffer can possibly be setup as a surface share by itself,
and thus not require game changes, but have not tested.  This is still suboptimal though, 
because we want to be able to beam to Oculus Go/Quest as well, for a larger market. The only
way to beam to those will be using the video encoder/decoder for a smaller bitstream.


After some research it also looks likely that we could just use Cuda directly.  It has direct
access to DX11 and DX9 surfaces, and a way to copy from one to the other, closer to a memcopy
than the freakish DX stuff.  And keep it all on the GPU.  For a local-only mode, this seems
like a much better path, because Cuda is a general purpose compute, not laden down with all
the graphics restrictions of DX.  

Should be fairly simple to just use Cuda to copy a registered DX9 buffer directly to a DX11
buffer.  There does not appear to be any Device restrictions there, but like always, hard to
say until you try it.  

Still, this is local-only, and a good backup plan, but does not get a stream out to Quest.  Not
sure that is important, but it would also allow us to get to Cardboard devices.  Maybe no one
cares. Probably.  But, unless there are limitations to the streaming approach, like terrible
latency or quality, this is a more general solution.

Of course... the streaming decode *also* requires Cuda for the decode.  But heading that way
now, to make a DX11 device that will receive the stream, decode it, and draw it.  Wish me luck.


1-17-19
Also worth considering is that the process space gets priority for the GPU.  In my Ball case,
the VR app was not getting GPU priority, because Ball was in front.  If I launch the VR app
in the same space, then it would get the same priority and no preemption.

This looks like it might be possible.  The OpenVR and OculusSDK can run as 32 bit applications.
They require x64 OS, but not for app.  That suggests we could create a VR runtime output
directly in the game process, and thus have it be at same priority as the game itself, and
avoid the attendant game launching problems.  

I have proven with the StereoUnproject modified test sample, that I can create DX9 and DX11
environments in the same app, without any problems.  Sharing data across the DX is non-trivial,
and currently am looking at nvcodec for this.  It looks like it might be possible to use
SharedSurfaces via the DXVA2 however, which is a fallback approach if nvcodec fails.  nvcodec
is a better approach, because we can then beam off device to wireless headsets as well. Performance
is the only real question for whether to use it or not.

If I launch a VR output in the game process space, that will be a standalone environment, and no
Unity support. That's partly bad because Unity brings UI features and Steam Workshop integration
for free.  It would simplify the runtime dramatically however.  This Deviare launch is weird. 


2-8-19
Finally, finally, figured out the nvcodec and and their SDK.  Took much longer than desired,
and motivation was low, but I finally did it.  The problem is that the decoder *requires* 
a file.  They do not expose the internal stream buffer they use to decode, and their API
is oriented around the sole concept of files.  This is remarkably stupid, but there are no
versions, not even the latest, that use anything as an input except files.  Everything is
opaque.  Sheeeet.

This is bad because that means that the only way to get the compressed bits to the DX11
side is to go through system RAM.  We can map a file to memory to avoid hard drive slowness, 
but that's RAM, not GPU memory.  This may or may not matter though, depending upon the 
performance. The H.264 encoder can drop the 2M pixels (8MB) of 1080p down to roughly 500K
at good quality.  PCI express can do something like 8MB/frame (500MB/60 frames).  That's quite
a bit more than needed, probably.  Pencils out, but real world will no doubt... vary.

Given the huge advantage of being able to stream off device to something like Go/Quest,
it's probably still worth finishing the sample using memory mapped file I/O.  Especially for
DX9, the games themselves will not be as demanding, so some overhead is going to be OK.


Next up was an investigation into Cuda copies.  Like everything I've looked at, this seems
like the way to go.  Probably get boned in the end too.  But, there is a direct interop
support for DX9 and DX11 in Cuda, for copying surface to surface, and it does not rely
upon Microsoft's lame surface sharing, it's GPU based using Cuda processor code. Closer to
a GPU memcpy.  

Initial look says this will work without too much trouble, and the API doesn't totally suck,
which is a refreshing change of pace.   Copying from backbuffer seems problematic, but from
a non-rendertarget it looks fine.  And keeps everything on the GPU like desired.

The only small gotcha is that the DX9 device *must* be an Ex device.  Otherwise the cuda calls
fail.  Even using an old SDK on modern hardware.  But, old SDK using just the change to
Direct3DCreate9Ex works.  The 'simpleD3D9' works, from the 7.0 cuda SDK, using non-Ex calls
for all the remaining pieces.  This maps directly to being able to switch to Ex call at init
for a given game, but no other change, which allows the game to run without all the glitched
APIs.


2-24-19
OK, finally, some real progress.  Going down the path of encode video to decode was a dead
end.  The stupid nvcodec *requires* a file to decode.  There is no support for a buffer 
based decode, which means that the data has to be sent back across PCI to the CPU to get to
the DX11 side.  This is maybe OK, performance of the compressed data can conceivably be OK,
especially by using a memory mapped file so it avoids HD hits.  But, lame.  

Given that, took a look at Cuda to see if it could do inter-op in any sensible fashion.  
APIs are 100x better than nvcodec, and SDK was usable instead of a piece of shit.  Looked
doable, so I built up a sample program from the stereounproject.  Same one as for nvcodec,
DX9 cubes being drawn, to a DX11 output window.  Made a new branch for the different approach.

Works!  Got it fully working, showing the DX9 game/app drawing the red cubes, and then making
a copy into a DX9 Surface, which is then cuda copied into a DX11 Texture2D.  Since both of 
those are mapped at once, it does an cudaMemcpyArrayToArray to get all the bits across.  From
there, the Texture2D is drawn to the Quad.  Fully functional, all data kept on the GPU.  A 
couple of extra copies using StretchRect, but that's not going to sting.

Now moving into incorporating this sample code into Katanga.